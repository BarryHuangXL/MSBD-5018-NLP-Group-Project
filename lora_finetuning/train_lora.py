import os
import json
from datasets import Dataset as HFDataset
from transformers import (
    AutoTokenizer, AutoModelForCausalLM,
    TrainingArguments, Trainer,
    DataCollatorForLanguageModeling, EarlyStoppingCallback
)
from peft import LoraConfig, get_peft_model, TaskType
import torch
import transformers

print("âœ… Transformers version:", transformers.__version__)

# ===== åŸºç¡€é…ç½® =====
MODEL_NAME = "Qwen/Qwen3-1.7B"
DATA_FILE = "train_final.jsonl"
BASE_OUTPUT_DIR = "./qwen3_1p7b_lora_multi_output"
MAX_LENGTH = 256
BATCH_SIZE = 5
GRAD_ACCUM = 8
LR = 5e-5
EPOCHS = 5
PATIENCE = 2  # æ—©5åœè½®æ¬¡
WEIGHT_DECAY = 0.01
DROPOUT = 0.1

# ===== æ•°æ®åŠ è½½ =====
def load_jsonl(path):
    data = []
    with open(path, 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip():
                data.append(json.loads(line))
    return data

raw_data = load_jsonl(DATA_FILE)
print(f"âœ… Loaded {len(raw_data)} samples")

# ===== æ„é€ è®­ç»ƒæ–‡æœ¬æ ¼å¼ =====
# é€‚é…ä½ çš„ä¸‰å­—æ®µæ ¼å¼ {"system": ..., "user": ..., "assistant": ...}
pairs = []
for item in raw_data:
    system = item.get("system", "")
    user = item.get("user", "")
    assistant = item.get("assistant", "")
    # æ‹¼æ¥ä¸ºå•æ¡è¾“å…¥è¾“å‡ºæ–‡æœ¬
    text = (
        f"<system>{system}</system>\n"
        f"<user>{user}</user>\n"
        f"<assistant>{assistant}</assistant>"
    )
    pairs.append({"text": text})

dataset = HFDataset.from_list(pairs)
split = dataset.train_test_split(test_size=0.1, seed=42)
train_ds, eval_ds = split["train"], split["test"]

# ===== Tokenizer & æ¨¡å‹åŠ è½½ =====
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

model = AutoModelForCausalLM.from_pretrained(
    MODEL_NAME,
    torch_dtype=torch.float16,
    device_map="auto",
    trust_remote_code=True,
)

# ===== LoRA æ–¹æ¡ˆï¼ˆå¯åˆ‡æ¢ï¼‰=====
LORA_SCHEMES = {
    "A_minimal": {
        "target_modules": ["q_proj", "v_proj"],
        "r": 4,
        "alpha": 8,
        "dropout": 0.05
    },
    "B_standard": {
        "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj"],
        "r": 8,
        "alpha": 16,
        "dropout": 0.05
    },
    "C_extended": {
        "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "up_proj", "down_proj"],
        "r": 16,
        "alpha": 32,
        "dropout": 0.1
    },
    "D_full_transform": {
        "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
        "r": 32,
        "alpha": 64,
        "dropout": 0.1
    },
    "E_regularized_small": {
        "target_modules": ["q_proj", "v_proj", "up_proj"],
        "r": 4,
        "alpha": 16,
        "dropout": 0.2
    }
}

# ===== é€‰æ‹©æ–¹æ¡ˆ =====
scheme_name = "A_minimal"
cfg = LORA_SCHEMES[scheme_name]
OUTPUT_DIR = os.path.join(BASE_OUTPUT_DIR, scheme_name)
os.makedirs(OUTPUT_DIR, exist_ok=True)
print(f"\nğŸ§© Using LoRA scheme: {scheme_name}")
print(f"ğŸ“‚ Output directory: {OUTPUT_DIR}\n")

# ===== LoRAé…ç½® =====
lora_config = LoraConfig(
    r=cfg["r"],
    lora_alpha=cfg["alpha"],
    lora_dropout=cfg["dropout"],
    bias="none",
    task_type=TaskType.CAUSAL_LM,
    target_modules=cfg["target_modules"]
)
model = get_peft_model(model, lora_config)
model.print_trainable_parameters()

# ===== Tokenize =====
def tokenize_function(examples):
    tok = tokenizer(
        examples["text"],
        truncation=True,
        max_length=MAX_LENGTH,
        padding="max_length",
    )
    tok["labels"] = tok["input_ids"].copy()
    return tok

train_ds = train_ds.map(tokenize_function, batched=True, remove_columns=["text"])
eval_ds = eval_ds.map(tokenize_function, batched=True, remove_columns=["text"])
data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)

# ===== è®­ç»ƒå‚æ•° =====
args = TrainingArguments(
    output_dir=OUTPUT_DIR,
    num_train_epochs=EPOCHS,
    learning_rate=LR,
    per_device_train_batch_size=BATCH_SIZE,
    per_device_eval_batch_size=BATCH_SIZE,
    gradient_accumulation_steps=GRAD_ACCUM,
    fp16=True,
    logging_steps=20,
    eval_strategy="epoch",
    save_strategy="epoch",
    save_total_limit=2,
    optim="adamw_torch",
    weight_decay=WEIGHT_DECAY,  # âœ… æ­£åˆ™åŒ–
    max_grad_norm=1.0,          # âœ… æ¢¯åº¦è£å‰ª
    load_best_model_at_end=True,
    metric_for_best_model="eval_loss",
    report_to="none",
    resume_from_checkpoint=True,  # å…³é”®å‚æ•°ï¼šè‡ªåŠ¨ä»æœ€æ–°checkpointæ¢å¤
)

# ===== Trainer + æ—©åœæœºåˆ¶ =====
trainer = Trainer(
    model=model,
    args=args,
    train_dataset=train_ds,
    eval_dataset=eval_ds,
    data_collator=data_collator,
    callbacks=[EarlyStoppingCallback(early_stopping_patience=PATIENCE)]
)

# ===== å¼€å§‹è®­ç»ƒ =====
trainer.train()

# ===== ä¿å­˜æ¨¡å‹ =====
adapter_dir = os.path.join(OUTPUT_DIR, f"lora_adapter_{scheme_name}")
model.save_pretrained(adapter_dir)
tokenizer.save_pretrained(adapter_dir)
print(f"ğŸ¯ LoRA adapter saved at: {adapter_dir}")

# ===== æ¨ç†æµ‹è¯• =====
def infer(prompt):
    model.eval()
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=128,
            temperature=0.7,
            top_p=0.9,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )
    print("\nğŸ§¾ Prompt:\n", prompt)
    print("\nğŸ’¬ Output:\n", tokenizer.decode(outputs[0], skip_special_tokens=True))

# ç¤ºä¾‹æµ‹è¯•
test_prompt = (
    "<system>You are a financial sentiment analysis expert. "
    "Your task is to analyze the sentiment expressed in the given text. "
    "Only reply with positive, neutral, or negative.</system>\n"
    "<user>The company reported record profits this quarter.</user>\n"
    "<assistant>"
)
infer(test_prompt)

