{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531a4cbc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "# File paths\n",
    "file_path = 'train.json'\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "# Store data for two system types separately\n",
    "system1_data = []  # First system: Financial expert\n",
    "system2_data = []  # Second system: Sentiment analysis expert\n",
    "\n",
    "print(\"Reading and categorizing data...\")\n",
    "\n",
    "# Read and categorize data\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        json_obj = json.loads(line.strip())\n",
    "        \n",
    "        if \"system\" in json_obj:\n",
    "            system_content = json_obj[\"system\"]\n",
    "            \n",
    "            # First system: Financial expert\n",
    "            if system_content == 'You are a financial expert. Your task is to provide accurate and insightful answers to finance-related questions based on the given conversation context.':\n",
    "                system1_data.append(json_obj)\n",
    "            \n",
    "            # Second system: Sentiment analysis expert\n",
    "            elif system_content == 'You are a financial sentiment analysis expert. Your task is to analyze the sentiment expressed in the given financial text.Only reply with positive, neutral, or negative.':\n",
    "                system2_data.append(json_obj)\n",
    "\n",
    "print(f\"Original data statistics:\")\n",
    "print(f\"First system data count: {len(system1_data)}\")\n",
    "print(f\"Second system data count: {len(system2_data)}\")\n",
    "\n",
    "# Step 1: Initial sampling from original data\n",
    "# Randomly sample 6000 from 38757 records of first system\n",
    "# Randomly sample 8000 from 28628 records of second system\n",
    "first_sample_system1 = random.sample(system1_data, 6000)\n",
    "first_sample_system2 = random.sample(system2_data, 8000)\n",
    "\n",
    "print(f\"\\nFirst sampling results:\")\n",
    "print(f\"Sampled from first system: {len(first_sample_system1)}\")\n",
    "print(f\"Sampled from second system: {len(first_sample_system2)}\")\n",
    "\n",
    "# Step 2: Further sampling for training set from first sample\n",
    "# Sample 4000 from 6000 as part of training set\n",
    "# Sample 6000 from 8000 as part of training set\n",
    "train_system1 = random.sample(first_sample_system1, 4000)\n",
    "train_system2 = random.sample(first_sample_system2, 6000)\n",
    "\n",
    "# Combine into final training set (4000 + 6000 = 10000)\n",
    "train_data = train_system1 + train_system2\n",
    "\n",
    "# Step 3: Remaining data as test set\n",
    "# Remaining from first system: 6000 - 4000 = 2000\n",
    "test_system1 = [item for item in first_sample_system1 if item not in train_system1]\n",
    "\n",
    "# Remaining from second system: 8000 - 6000 = 2000  \n",
    "test_system2 = [item for item in first_sample_system2 if item not in train_system2]\n",
    "\n",
    "# Combine test set (2000 + 2000 = 4000)\n",
    "test_data = test_system1 + test_system2\n",
    "random.shuffle(train_data) \n",
    "\n",
    "print(f\"\\nFinal data split results:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Training set total: {len(train_data)}\")\n",
    "print(f\"  - First system: {len(train_system1)}\")\n",
    "print(f\"  - Second system: {len(train_system2)}\")\n",
    "\n",
    "print(f\"\\nTest set total: {len(test_data)}\")\n",
    "print(f\"  - First system: {len(test_system1)}\")\n",
    "print(f\"  - Second system: {len(test_system2)}\")\n",
    "\n",
    "print(f\"\\nUnused data remaining:\")\n",
    "print(f\"  - First system remaining: {len(system1_data) - 6000}\")\n",
    "print(f\"  - Second system remaining: {len(system2_data) - 8000}\")\n",
    "\n",
    "# Save training set as jsonl format\n",
    "with open('train_final.jsonl', 'w', encoding='utf-8') as f:\n",
    "    for item in train_data:\n",
    "        f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "\n",
    "# Save test set as jsonl format\n",
    "with open('test_final.jsonl', 'w', encoding='utf-8') as f:\n",
    "    for item in test_data:\n",
    "        f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "\n",
    "# Optionally save test sets for each task separately as jsonl format\n",
    "with open('test_task1.jsonl', 'w', encoding='utf-8') as f:\n",
    "    for item in test_system1:\n",
    "        f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "\n",
    "with open('test_task2.jsonl', 'w', encoding='utf-8') as f:\n",
    "    for item in test_system2:\n",
    "        f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "\n",
    "print(\"\\nFiles saved:\")\n",
    "print(\"- train_final.jsonl: Training set (10,000)\")\n",
    "print(\"- test_final.jsonl: Test set (4,000)\")\n",
    "print(\"- test_task1.jsonl: Task 1 test set (2,000)\")\n",
    "print(\"- test_task2.jsonl: Task 2 test set (2,000)\")\n",
    "\n",
    "# Validate data integrity\n",
    "print(f\"\\nValidation information:\")\n",
    "print(f\"Training set + test set total: {len(train_data) + len(test_data)}\")\n",
    "print(f\"Total sampled from original data: {6000 + 8000}\")\n",
    "print(f\"Data split correctness: {(len(train_data) + len(test_data)) == 14000}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
